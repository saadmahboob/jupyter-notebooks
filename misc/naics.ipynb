{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Industry to NAICS Data Set for Supervised Learning\n",
    "\n",
    "https://www.census.gov/eos/www/naics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import difflib\n",
    "import sys\n",
    "import time \n",
    "import pickle \n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2041 entries, 0 to 2195\n",
      "Data columns (total 3 columns):\n",
      "Code           2041 non-null object\n",
      "Title          2041 non-null object\n",
      "Description    2041 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 63.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Excel file with 2017 NAICS codes \n",
    "# Source: https://www.census.gov/eos/www/naics/downloadables/downloadables.html\n",
    "df = pd.read_excel('2017_NAICS_Descriptions.xlsx')\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].str.strip()\n",
    "df['Title'] = df['Title'].apply(lambda title: title[:-1] if title.endswith('T') else title)\n",
    "\n",
    "df['Description'] = df['Description'].apply(lambda desc: desc[:desc.find('Cross-Reference')] if 'Cross-Reference' in desc else desc)\n",
    "df['Description'] = df['Description'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agriculture, Forestry, Fishing and Hunting\n",
      "******************************************\n",
      "\n",
      "The Sector as a Whole\n",
      "\n",
      "The Agriculture, Forestry, Fishing and Hunting sector comprises establishments primarily engaged in growing crops, raising animals, harvesting timber, and harvesting fish and other animals from a farm, ranch, or their natural habitats.\n",
      "\n",
      "The establishments in this sector are often described as farms, ranches, dairies, greenhouses, nurseries, orchards, or hatcheries.  A farm may consist of a single tract of land or a number of separate tracts which may be held under different tenures.  For example, one tract may be owned by the farm operator and another rented.  It may be operated by the operator alone or with the assistance of members of the household or hired employees, or it may be operated by a partnership, corporation, or other type of organization. When a landowner has one or more tenants, renters, croppers, or managers, the land operated by each is considered a farm.\n",
      "\n",
      "The sector distinguishes two basic activities: agricultural production and agricultural support activities.  Agricultural production includes establishments performing the complete farm or ranch operation, such as farm owner-operators and tenant farm operators.  Agricultural support activities include establishments that perform one or more activities associated with farm operation, such as soil preparation, planting, harvesting, and management, on a contract or fee basis.\n",
      "\n",
      "Excluded from the Agriculture, Forestry, Fishing and Hunting sector are establishments primarily engaged in agricultural research and establishments primarily engaged in administering programs for regulating and conserving land, mineral, wildlife, and forest use.  These establishments are classified in Industry 54171, Research and Development in the Physical, Engineering, and Life Sciences; and Industry 92412, Administration of Conservation Programs, respectively.\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(df.iloc[index].Title)\n",
    "print('*'*len(df.iloc[index].Title) + '\\n')\n",
    "print(df.iloc[index].Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Alphabetical Index of the NAICS Document For More Mappings\n",
    "\n",
    "Source: https://www.census.gov/eos/www/naics/2017NAICS/2017_NAICS_Manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 963\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdfFileObj = open('naics_book.pdf','rb')     #'rb' for read binary mode\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "print('Number of pages:', pdfReader.numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"653\\n \\n \\ncensus.gov/naics\\n \\nAlphabetic Index\\n \\n \\n311611\\n \\nAbattoirs\\n \\n621410\\n \\nAbortion clinics\\n \\n334519\\n \\nAbrasion testing machines manufacturing\\n \\n339114\\n \\nAbrasive points, wheels, and disks, dental, \\nmanufacturing\\n \\n327910\\n \\nAbrasive products manufacturing\\n \\n212322\\n \\nAbrasive sand quarrying and/or \\nbeneficiating\\n \\n212399\\n \\nAbrasive stones (e.g., emery, grindstones, \\nhones, pumice) mining and/or beneficiating\\n \\n423840\\n \\nAbrasives merchant wholesalers\\n \\n212399\\n \\nAbrasives, natural, mining and/or \\nbeneficiating\\n \\n322121\\n \\nAbsorbent paper stock manufacturing\\n \\n332420\\n \\nAbsorbers, gas, heavy gauge metal, \\nmanufacturing\\n \\n334513\\n \\nAbsorption analyzers, industrial process \\ntype (e.g., infrared), manufacturing\\n \\n237310\\n \\nAbutment construction\\n \\n611691\\n \\nAcademic tutoring services\\n \\n611310\\n \\nAcademies, college or university\\n \\n611110\\n \\nAcademies, elementary or secondary\\n \\n611210\\n \\nAcademies, junior college\\n \\n611310\\n \\nAcademies, military service (college)\\n \\n611620\\n \\nAcademies, riding instruction\\n \\n334511\\n \\nAcceleration indicators and systems \\ncomponents, \\naerospace type, \\nmanufacturing\\n \\n325199\\n \\nAccelerators (i.e., basic synthetic chemical) \\nmanufacturing\\n \\n334519\\n \\nAccelerometers (except aerospace type) \\nmanufacturing\\n \\n238330\\n \\nAccess flooring installation\\n \\n424320\\n \\nAccessories, clothing, men's and boy's, \\nmerchant \\nwholesalers\\n \\n813319\\n \\nAccident prevention associations\\n \\n524130\\n \\nAccidental and health reinsurance carriers\\n \\n524113\\n \\nAccidental death and dismemberment \\ninsurance carriers, direct\\n \\n524113\\n \\nAccidental death and dismemberment \\ninsurance underwriting, direct\\n \\n339992\\n \\nAccordions and parts manufacturing\\n \\n561440\\n \\nAccount collection services\\n \\n541219\\n \\nAccountants' (except CPAs) offices\\n \\n541219\\n \\nAccountants' (except CPAs) private \\npractices\\n \\n541211\\n \\nAccountants' (i.e., CPAs) offices, certified \\npublic\\n \\n541211\\n \\nAccountants' (i.e., \\nCPAs) private practices, \\ncertified public\\n \\n813920\\n \\nAccountants' associations\\n \\n541211\\n \\nAccounting (i.e., CPAs) services, certified \\npublic\\n \\n423420\\n \\nAccounting machines merchant \\nwholesalers\\n \\n541219\\n \\nAccounting services (except CPAs)\\n \\n332420\\n \\nAccumulators, \\nindustrial pressure vessels, \\nheavy gauge metal, manufacturing\\n \\n325211\\n \\nAcetal resins manufacturing\\n \\n325199\\n \\nAcetaldehyde manufacturing\\n \\n325220\\n \\nAcetate fibers and filaments manufacturing\\n \\n325194\\n \\nAcetate of lime, natural, made by \\ndistillation of wood\\n \\n313110\\n \\nAcetate spun yarns made from purchased \\nfiber\\n \\n325199\\n \\nAcetates, not specified elsewhere by \\nprocess, manufacturing\\n \\n325199\\n \\nAcetic acid manufacturing\\n \\n325199\\n \\nAcetic anhydride manufacturing\\n \\n325199\\n \\nAcetin manufacturing\\n \\n325194\\n \\nAcetone, natural, manufacturing\\n \\n325199\\n \\nAcetone, synthetic, manufacturing\\n \\n332420\\n \\nAcetylene cylinders, heavy gauge metal, \\nmanufacturing\\n \\n325120\\n \\nAcetylene manufacturing\\n \\n325411\\n \\nAcetylsalicylic acid manufacturing\\n \\n325130\\n \\nAcid dyes, synthetic organic, \\nmanufacturing\\n \\n325199\\n \\nAcid esters, not \\nspecified elsewhere by \\nprocess, manufacturing\\n \\n324110\\n \\nAcid oils made in petroleum refineries\\n \\n236210\\n \\nAcid plant construction\\n \\n562211\\n \\nAcid waste disposal facilities\\n \\n562211\\n \\nAcid waste treatment facilities\\n \\n334513\\n \\nAcidity (i.e., pH) instruments, industrial \\nprocess type, manufacturing\\n \\n334516\\n \\nAcidity (i.e., pH) measuring equipment, \\nlaboratory analysis\\n-\\ntype, manufacturing\\n \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the index code descriptions\n",
    "start_page = 654\n",
    "end_page = 963\n",
    "\n",
    "pageObj = pdfReader.getPage(start_page)    \n",
    "text = pageObj.extractText()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('311611', 'Abattoirs'),\n",
       " ('621410', 'Abortion clinics'),\n",
       " ('334519', 'Abrasion testing machines manufacturing'),\n",
       " ('339114', 'Abrasive points, wheels, and disks, dental, '),\n",
       " ('327910', 'Abrasive products manufacturing')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract code to industry mappings\n",
    "pattern = re.compile(r'\\n(\\d{6})\\n\\s+\\n(.*)\\n')\n",
    "data = re.findall(pattern, text)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = []\n",
    "titles = []\n",
    "\n",
    "for page in range(start_page, end_page):\n",
    "    pageObj = pdfReader.getPage(page)    \n",
    "    text = pageObj.extractText()\n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        code, title = match\n",
    "        codes.append(code)\n",
    "        # strip the title of end punctuation\n",
    "        title = title.strip(string.punctuation + string.whitespace)\n",
    "        titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22066</th>\n",
       "      <td>339992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zithers and parts manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22067</th>\n",
       "      <td>925120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zoning boards and commissions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22068</th>\n",
       "      <td>712130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zoological gardens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22069</th>\n",
       "      <td>712130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22070</th>\n",
       "      <td>111219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zucchini farming, field, bedding plant and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Code Description                                       Title\n",
       "22066  339992         NaN             Zithers and parts manufacturing\n",
       "22067  925120         NaN               Zoning boards and commissions\n",
       "22068  712130         NaN                          Zoological gardens\n",
       "22069  712130         NaN                                        Zoos\n",
       "22070  111219         NaN  Zucchini farming, field, bedding plant and"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append the appendix data to the excel file\n",
    "apendix_df = pd.DataFrame({'Code': codes, 'Title': titles})\n",
    "out_df = df.append(apendix_df, ignore_index=True)\n",
    "out_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "out_df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenization\n",
    "\n",
    "Source: https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, special=None,\n",
    "                 punct=None, lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.special    = special\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(word_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip(string.punctuation) if self.strip else token\n",
    "                \n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "                    \n",
    "                # If token is a number continue\n",
    "                if token.isdecimal():\n",
    "                    continue\n",
    "                \n",
    "                # If token is a special word, continue\n",
    "                if self.special and token in self.special:\n",
    "                    continue\n",
    "                \n",
    "                    \n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                    \n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary Based on The NAICS document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22071 entries, 0 to 22070\n",
      "Data columns (total 3 columns):\n",
      "Code           22071 non-null object\n",
      "Description    2041 non-null object\n",
      "Title          22071 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 689.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data.csv', index_col=0)\n",
    "print(df.info())\n",
    "\n",
    "titles = df['Title'].dropna()\n",
    "descriptions = df['Description'].dropna()\n",
    "\n",
    "sentences = pd.concat([titles, descriptions], axis=0)\n",
    "preprocessor = NLTKPreprocessor(special=[\n",
    "    'company', 'business', 'manufacturing', 'facility',\n",
    "    'start',   'build',    \n",
    "])\n",
    "\n",
    "descriptions = preprocessor.transform(descriptions)\n",
    "titles = preprocessor.transform(titles)\n",
    "\n",
    "labels = set(word for lst in titles for word in lst)\n",
    "ordinary = set(word for lst in descriptions for word in lst)\n",
    "vocabulary = labels.union(ordinary)\n",
    "\n",
    "'liability' in labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search API for Assigning NAICS codes to User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv('user_data.csv', header=None)[0]\n",
    "user_df = user_df.str.strip().dropna()\n",
    "tokens = preprocessor.transform(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(text, patterns, limit=5):\n",
    "    \"\"\"Scrape NAICS codes for `text` from returned Google Search page.\n",
    "    \"\"\"\n",
    "    results = google.search(text + ' ' + 'NAICS code', 1)\n",
    "    result = []\n",
    "    for pattern in patterns:\n",
    "        matches = [pattern.search(result.name) for result in results]\n",
    "        codes = [match.group(1) for match in matches if match is not None]\n",
    "        result.extend(codes[:limit-len(result)])\n",
    "        \n",
    "        if len(result) == limit:\n",
    "            break\n",
    "    \n",
    "    if len(result) < limit:\n",
    "        result.extend([None for i in range(limit-len(result))])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Regexp for scraping the Google Search page\n",
    "STATCAT = re.compile(r'NAICS\\s+Code\\s+(\\d+)')\n",
    "CENSUS = re.compile(r'NAICS:\\s+(\\d+)')\n",
    "patterns = [STATCAT, CENSUS]\n",
    "\n",
    "# In case downloading is interrupted, \n",
    "# start from this sample.\n",
    "start=7200\n",
    "for j, words in enumerate(tokens[start:]):\n",
    "    if j % 100 == 0 and j > 0:\n",
    "        with open('pickles/%d.p' % (j+start), 'wb') as f:\n",
    "            print('writing results to file...')\n",
    "            pickle.dump(results, f)\n",
    "            results = []\n",
    "            \n",
    "    text = ' '.join(words)\n",
    "    print('%d: %s --' % ((j+start), text), end=' ')\n",
    "    \n",
    "    tick = time.time()\n",
    "    codes = scrape(text, patterns)\n",
    "    if any(val is not None for val in codes):\n",
    "        results.append((text, codes))\n",
    "        tock = time.time()\n",
    "        print('%.2f sec' % (tock-tick))\n",
    "        continue\n",
    "    \n",
    "    print('Closer look at', text)\n",
    "    # use the words most relevant to the NAICS encoding\n",
    "    processed = []\n",
    "    for word in words:\n",
    "        similar = difflib.get_close_matches(word, vocabulary, n=1)\n",
    "        processed.extend(similar)\n",
    "        \n",
    "    if not processed:\n",
    "        continue\n",
    "        \n",
    "    stride = max(1, len(processed) // 3)\n",
    "    for i in range(0, len(processed), stride):\n",
    "        text = ' '.join(processed[i:i+stride])\n",
    "        print('%d: %s --' % ((j+start), text), end=' ')\n",
    "        codes = scrape(text, patterns)\n",
    "        results.append((text, codes))  \n",
    "    \n",
    "    tock = time.time()\n",
    "    \n",
    "    print('%.2f sec' % (tock-tick))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User Text To NAICS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './pickles'\n",
    "data = []\n",
    "\n",
    "for pfile in os.listdir(directory):\n",
    "    path = os.path.join(directory, pfile)\n",
    "    with open(path, 'rb') as f:\n",
    "        data.extend(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the NAICS codes to the first 5 (industry level) digits\n",
    "n = 5\n",
    "results = []\n",
    "\n",
    "for query, code_list in data:\n",
    "    truncodes = [code[:n] for code in code_list if code is not None]\n",
    "    counts = Counter(truncodes)\n",
    "    if len(counts) > 0:\n",
    "        try:\n",
    "            likely = max(counts.items(), key=operator.itemgetter(1))[0]\n",
    "            results.append((query, likely))\n",
    "        except ValueError as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(results, columns=['Description', 'Code'])\n",
    "print(user_df.info())\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.to_csv('naics-google-search-v1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
